{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scala Spark: Load to Data Lake"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\nimport spark.implicits._\nyellowSourcePath: String = wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2018/puMonth=*/*.parquet\ntaxiZoneSourcePath: String = abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone_lookup.csv\ntaxiZonePath: String = abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone\ntaxiRatePath: String = abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_rate_code\nyellowDeltaPath: String = abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/tripdata/yellow_delta\ndateFormat: String = yyyy-MM-dd HH:mm:ss\ntaxiZoneSchema: org.apache.spark.sql.types.StructType = StructType(StructField(LocationID,IntegerType,true), StructField(Borough,StringType,true), StructField(Zone,StringType,true), StructField(ServiceZone,StringType,true))"
          },
          "execution_count": 8,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\n",
        "\n",
        "import spark.implicits._\n",
        "\n",
        "val yellowSourcePath = \"wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2018/puMonth=*/*.parquet\"\n",
        "val taxiZoneSourcePath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone_lookup.csv\"\n",
        "\n",
        "val taxiZonePath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone\"\n",
        "val taxiRatePath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_rate_code\"\n",
        "val yellowDeltaPath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/tripdata/yellow_delta\"\n",
        "\n",
        "val dateFormat = \"yyyy-MM-dd HH:mm:ss\"\n",
        "\n",
        "// Define a schema that Spark understands. This is one of several ways to do it.\n",
        "val taxiZoneSchema = StructType(Seq(\n",
        "    StructField(\"LocationID\", IntegerType),\n",
        "    StructField(\"Borough\", StringType),\n",
        "    StructField(\"Zone\", StringType),\n",
        "    StructField(\"ServiceZone\", StringType)\n",
        "))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "zoneDF: org.apache.spark.sql.DataFrame = [LocationID: int, Borough: string ... 2 more fields]\n+----------+-------------+--------------------+-----------+\n|LocationID|      Borough|                Zone|ServiceZone|\n+----------+-------------+--------------------+-----------+\n|         1|          EWR|      Newark Airport|        EWR|\n|         2|       Queens|         Jamaica Bay|  Boro Zone|\n|         3|        Bronx|Allerton/Pelham G...|  Boro Zone|\n|         4|    Manhattan|       Alphabet City|Yellow Zone|\n|         5|Staten Island|       Arden Heights|  Boro Zone|\n|         6|Staten Island|Arrochar/Fort Wad...|  Boro Zone|\n|         7|       Queens|             Astoria|  Boro Zone|\n|         8|       Queens|        Astoria Park|  Boro Zone|\n|         9|       Queens|          Auburndale|  Boro Zone|\n|        10|       Queens|        Baisley Park|  Boro Zone|\n|        11|     Brooklyn|          Bath Beach|  Boro Zone|\n|        12|    Manhattan|        Battery Park|Yellow Zone|\n|        13|    Manhattan|   Battery Park City|Yellow Zone|\n|        14|     Brooklyn|           Bay Ridge|  Boro Zone|\n|        15|       Queens|Bay Terrace/Fort ...|  Boro Zone|\n|        16|       Queens|             Bayside|  Boro Zone|\n|        17|     Brooklyn|             Bedford|  Boro Zone|\n|        18|        Bronx|        Bedford Park|  Boro Zone|\n|        19|       Queens|           Bellerose|  Boro Zone|\n|        20|        Bronx|             Belmont|  Boro Zone|\n+----------+-------------+--------------------+-----------+\nonly showing top 20 rows"
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val zoneDF = spark.read.option(\"header\",\"true\").schema(taxiZoneSchema).csv(taxiZoneSourcePath) \n",
        "\n",
        "zoneDF.write.format(\"delta\").mode(\"overwrite\").save(taxiZonePath)\n",
        "\n",
        "zoneDF.show()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "inputDF: org.apache.spark.sql.DataFrame = [vendorID: string, tpepPickupDateTime: timestamp ... 19 more fields]\ntransformedDF: org.apache.spark.sql.DataFrame = [vendorID: string, tpepPickupDateTime: timestamp ... 23 more fields]\nzoneDF: org.apache.spark.sql.DataFrame = [LocationID: int, Borough: string ... 2 more fields]\ntripDF: org.apache.spark.sql.DataFrame = [vendorID: string, tpepPickupDateTime: timestamp ... 26 more fields]"
          },
          "execution_count": 14,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val inputDF = spark.read.parquet(yellowSourcePath)\n",
        "\n",
        "// Take your pick on how to transform, withColumn or SQL Expressions. Only one of these is needed.\n",
        "\n",
        "// Option A\n",
        "// val transformedDF = {\n",
        "//     inputDF\n",
        "//      .withColumn(\"yearMonth\", regexp_replace(substring(\"tpepPickupDatetime\",1,7), '-', '_'))\n",
        "//      .withColumn(\"pickupDt\", to_date(\"tpepPickupDatetime\", dateFormat)) \n",
        "//      .withColumn(\"dropoffDt\", to_date(\"tpepDropoffDatetime\", dateFormat))\n",
        "//      .withColumn(\"tipPct\", col(\"tipAmount\") / col(\"totalAmount\"))\n",
        "// }\n",
        "\n",
        "// Option B\n",
        "val transformedDF = inputDF.selectExpr(\n",
        "                  \"*\",\n",
        "                  \"replace(left(tpepPickupDatetime, 7),'-','_') as yearMonth\",\n",
        "                  s\"to_date(tpepPickupDatetime, '$dateFormat') as pickupDt\",\n",
        "                  s\"to_date(tpepDropoffDatetime, '$dateFormat') as dropoffDt\",\n",
        "                  \"tipAmount/totalAmount as tipPct\")\n",
        "\n",
        "val zoneDF = spark.read.format(\"delta\").load(taxiZonePath)\n",
        "\n",
        "// Join to bring in Taxi Zone data\n",
        "val tripDF = {\n",
        "    transformedDF.as(\"t\")\n",
        "        .join(zoneDF.as(\"z\"), expr(\"t.PULocationID == z.LocationID\"), joinType=\"left\").drop(\"LocationID\")\n",
        "        .withColumnRenamed(\"Burough\", \"PickupBurrough\")\n",
        "        .withColumnRenamed(\"Zone\", \"PickupZone\")\n",
        "        .withColumnRenamed(\"ServiceZone\", \"PickupServiceZone\")\n",
        "}\n",
        "\n",
        "tripDF.write.mode(\"overwrite\").partitionBy(\"yearMonth\").format(\"delta\").save(yellowDeltaPath)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test read\n",
        "Simple test read of the delta formatted data that was just saved."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "testDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [vendorID: string, tpepPickupDateTime: timestamp ... 26 more fields]\n+--------+-------------------+-------------------+--------------+\n|VendorID| tpepPickupDatetime|tpepDropoffDatetime|passengerCount|\n+--------+-------------------+-------------------+--------------+\n|       2|2018-03-06 09:12:00|2018-03-06 09:29:46|             1|\n|       2|2018-03-24 17:42:04|2018-03-25 01:04:03|             4|\n|       2|2018-03-31 05:57:24|2018-03-31 06:19:42|             1|\n|       2|2018-03-02 10:34:37|2018-03-02 10:39:59|             1|\n|       2|2018-03-05 22:26:30|2018-03-05 22:42:32|             1|\n|       2|2018-03-02 00:37:08|2018-03-02 00:45:17|             1|\n|       2|2018-03-31 11:21:32|2018-03-31 11:29:36|             1|\n|       1|2018-03-02 03:27:16|2018-03-02 03:29:38|             1|\n|       2|2018-03-05 20:19:56|2018-03-05 20:26:06|             2|\n|       1|2018-03-02 05:43:37|2018-03-02 05:48:21|             1|\n|       1|2018-03-31 12:33:33|2018-03-31 12:59:11|             2|\n|       1|2018-03-02 07:26:24|2018-03-02 07:34:55|             1|\n|       2|2018-03-06 04:32:46|2018-03-06 04:41:05|             1|\n|       2|2018-03-02 08:35:35|2018-03-02 08:39:25|             1|\n|       2|2018-03-31 08:31:53|2018-03-31 08:35:32|             5|\n|       1|2018-03-02 04:03:44|2018-03-02 04:16:33|             2|\n|       2|2018-03-05 20:09:34|2018-03-05 20:17:54|             2|\n|       2|2018-03-02 11:23:53|2018-03-02 11:28:21|             1|\n|       2|2018-03-31 12:47:56|2018-03-31 13:06:39|             5|\n|       1|2018-03-02 06:46:06|2018-03-02 06:49:16|             1|\n+--------+-------------------+-------------------+--------------+"
          },
          "execution_count": 16,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val testDF = spark.read.format(\"delta\").load(yellowDeltaPath).limit(20)\n",
        "testDF.select(\"VendorID\", \"tpepPickupDatetime\", \"tpepDropoffDatetime\", \"passengerCount\").show()"
      ],
      "attachments": {}
    }
  ]
}